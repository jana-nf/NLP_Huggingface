{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTCC90GKe8uMW7lbrXVnc3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jana-nf/NLP_Huggingface/blob/main/CO2_Emissions_Hub_Jana_nf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CO2 Emissions and the ðŸ¤— Hub: Leading the Charge\n",
        "\n",
        "What are CO2 Emissions and why are they important?\n",
        "\n",
        "Climate change is one of the greatest challenges that we are facing and reducing emissions of greenhouse gases such as carbon dioxide (CO2) is an important part of tackling this problem.\n",
        "\n",
        "Training and deploying machine learning models will emit CO2 due to the energy usage of the computing infrastructures that are used: from GPUs to storage, it all needs energy to function and emits CO2 in the process."
      ],
      "metadata": {
        "id": "JdUSO7W3HnBS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkY6GYmLHDwd",
        "outputId": "86f76b4a-29e0-4e12-d82e-3412b9497199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.29.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "pip install huggingface_hub -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "models = api.list_models(emissions_thresholds=(None, 100), card_data=True)\n",
        "len(models)\n",
        ">>> 191\n"
      ],
      "metadata": {
        "id": "Qq1SKudAIDjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models[0]\n",
        "print(f'Model Name: {model.modelId}\\nCO2 Emitted during training: {model.cardData[\"co2_eq_emissions\"]}')\n",
        "\n",
        ">>> Model Name: esiebomajeremiah/autonlp-email-classification-657119381\n",
        "    CO2 Emitted during training: 3.516233232503715\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Xsr3CKidIWO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = api.list_models(emissions_thresholds=(500, None), cardData=True)\n",
        "len(models)\n",
        ">>> 10\n"
      ],
      "metadata": {
        "id": "AmzOdj0II1lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models[0]\n",
        "print(f'Model Name: {model.modelId}\\nCO2 Emitted during training: {model.cardData[\"co2_eq_emissions\"]}')\n",
        "\n",
        ">>> Model Name: Maltehb/aelaectra-danish-electra-small-cased\n",
        "    CO2 Emitted during training: 4009.5\n"
      ],
      "metadata": {
        "id": "_6IsGnuzI8nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "â€‹\n",
        "ds = load_dataset(\"imdb\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "â€‹\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "â€‹\n",
        "â€‹\n",
        "small_train_dataset = ds[\"train\"].shuffle(seed=42).select(range(1000)).map(tokenize_function, batched=True)\n",
        "small_eval_dataset = ds[\"test\"].shuffle(seed=42).select(range(1000)).map(tokenize_function, batched=True)\n",
        "â€‹\n",
        "â€‹\n",
        "training_args = TrainingArguments(\n",
        "    \"codecarbon-text-classification\",\n",
        "    num_train_epochs=4,\n",
        "    push_to_hub=True\n",
        ")\n",
        "â€‹\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        ")\n",
        "â€‹\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9Ejk0SjbJAPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_yYjBhUqJFWB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
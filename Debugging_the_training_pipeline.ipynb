{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jana-nf/NLP_Huggingface/blob/main/Debugging_the_training_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc13qEx1ia2P"
      },
      "source": [
        "# Debugging the training pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb0_aSVHia2q"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKTitUxOia2-"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7FBm_hBia3H",
        "outputId": "a7252671-4828-401c-fdfe-68136c2105d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ValueError: You have to specify either input_ids or inputs_embeds'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"distilbert-finetuned-mnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mnli\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=raw_datasets[\"train\"],\n",
        "    eval_dataset=raw_datasets[\"validation_matched\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pz8HrRN8ia3U",
        "outputId": "aab66cb3-abe5-46c6-e969-9cc2ef6030ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hypothesis': 'Product and geography are what make cream skimming work. ',\n",
              " 'idx': 0,\n",
              " 'label': 1,\n",
              " 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4FACHXvia3Y",
        "outputId": "d30c7c0c-3648-41a9-c1cf-05a06f66fda7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ValueError: expected sequence of length 43 at dim 1 (got 37)'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"distilbert-finetuned-mnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mnli\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DwwJFEpia3d",
        "outputId": "1bbd0b40-f361-498a-f94a-f2c47a82e8a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP]'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5MLjDJkia3l",
        "outputId": "042bea88-2d8f-4383-a76a-bca93e2a6fc4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['attention_mask', 'hypothesis', 'idx', 'input_ids', 'label', 'premise'])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvW7DbT5ia3n",
        "outputId": "00df6316-da0d-4b49-eaed-777f757d169d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(trainer.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HqenTTqia3r",
        "outputId": "7b00ac64-e698-40aa-af0a-dd726572093a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset[0][\"attention_mask\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgE7BOkcia32",
        "outputId": "b858778b-d390-4c15-c3a1-aacfd6c78c74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(trainer.train_dataset[0][\"attention_mask\"]) == len(\n",
        "    trainer.train_dataset[0][\"input_ids\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j10I6PQSia37",
        "outputId": "dd303dde-0e14-4264-8b46-781e8b5ca091"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset[0][\"label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVklGhTmia3-",
        "outputId": "74fe93bb-2636-481a-a581-839d78153950"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['entailment', 'neutral', 'contradiction']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset.features[\"label\"].names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EdQXlpNia4L",
        "outputId": "5722eea5-057c-43eb-b9e7-b00d7344634c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)\n",
              "    105                 batch[k] = torch.stack([f[k] for f in features])\n",
              "    106             else:\n",
              "--> 107                 batch[k] = torch.tensor([f[k] for f in features])\n",
              "    108 \n",
              "    109     return batch\n",
              "\n",
              "ValueError: expected sequence of length 45 at dim 1 (got 76)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for batch in trainer.get_train_dataloader():\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAFrzS8wia4O",
        "outputId": "034e2d7d-938e-4f9e-c43e-d7d1e096a46c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function transformers.data.data_collator.default_data_collator(features: List[InputDataClass], return_tensors='pt') -> Dict[str, Any]>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_collator = trainer.get_train_dataloader().collate_fn\n",
        "data_collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zixzoIDkia4d",
        "outputId": "7b1c995a-4c87-4fa7-d9b1-8aa518918d82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"distilbert-finetuned-mnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mnli\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTIb7NZEia4g"
      },
      "outputs": [],
      "source": [
        "data_collator = trainer.get_train_dataloader().collate_fn\n",
        "batch = data_collator([trainer.train_dataset[i] for i in range(4)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFv0wWuxia4k"
      },
      "outputs": [],
      "source": [
        "data_collator = trainer.get_train_dataloader().collate_fn\n",
        "actual_train_set = trainer._remove_unused_columns(trainer.train_dataset)\n",
        "batch = data_collator([actual_train_set[i] for i in range(4)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dNAsFjcia4m"
      },
      "outputs": [],
      "source": [
        "for batch in trainer.get_train_dataloader():\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VG8hbspia4n",
        "outputId": "90d28c55-336b-43a6-be01-77789227d737"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)\n",
              "   2386         )\n",
              "   2387     if dim == 2:\n",
              "-> 2388         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n",
              "   2389     elif dim == 4:\n",
              "   2390         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n",
              "\n",
              "IndexError: Target 2 is out of bounds."
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = trainer.model.cpu()(**batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-iQJ5Y7ia4o",
        "outputId": "7b672cfe-1dca-47a5-9614-b30b1a7b9efc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.model.config.num_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1McyChgia4t"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"distilbert-finetuned-mnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mnli\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmBi8Uehia41"
      },
      "outputs": [],
      "source": [
        "for batch in trainer.get_train_dataloader():\n",
        "    break\n",
        "\n",
        "outputs = trainer.model.cpu()(**batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfimzlRiia45"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "outputs = trainer.model.to(device)(**batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6X_U7yoia49"
      },
      "outputs": [],
      "source": [
        "loss = outputs.loss\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIhdTX2kia5A"
      },
      "outputs": [],
      "source": [
        "trainer.create_optimizer()\n",
        "trainer.optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUIbZsIzia5B",
        "outputId": "7799b34d-aea3-490b-e4a7-1ea7105aafc5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TypeError: only size-1 arrays can be converted to Python scalars"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This will take a long time and error out, so you shouldn't run this cell\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vfx7X454ia5L",
        "outputId": "354b2a2c-fb50-4727-e12a-368996a85ba1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TypeError: only size-1 arrays can be converted to Python scalars"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTdJkSKkia5e"
      },
      "outputs": [],
      "source": [
        "for batch in trainer.get_eval_dataloader():\n",
        "    break\n",
        "\n",
        "batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = trainer.model(**batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1liTp7-gia5f",
        "outputId": "8ca40965-f4d0-4b4f-c94e-2958bb62397f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TypeError: only size-1 arrays can be converted to Python scalars"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = outputs.logits.cpu().numpy()\n",
        "labels = batch[\"labels\"].cpu().numpy()\n",
        "\n",
        "compute_metrics((predictions, labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRvN-Z9Lia5h",
        "outputId": "1997a173-429d-4817-90dd-c11a66c3c53c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((8, 3), (8,))"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions.shape, labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teW_jkiQia5k",
        "outputId": "17d9d284-1a4a-44b6-db75-a8e9c5e28935"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.625}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "compute_metrics((predictions, labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1JaIV1wia5r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"distilbert-finetuned-mnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mnli\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fMvqFBQia5y"
      },
      "outputs": [],
      "source": [
        "for batch in trainer.get_train_dataloader():\n",
        "    break\n",
        "\n",
        "batch = {k: v.to(device) for k, v in batch.items()}\n",
        "trainer.create_optimizer()\n",
        "\n",
        "for _ in range(20):\n",
        "    outputs = trainer.model(**batch)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    trainer.optimizer.step()\n",
        "    trainer.optimizer.zero_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vh5rx16lia52",
        "outputId": "9897086b-0442-4c1e-9619-8749bc2f52da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 1.0}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = trainer.model(**batch)\n",
        "preds = outputs.logits\n",
        "labels = batch[\"labels\"]\n",
        "\n",
        "compute_metrics((preds.cpu().numpy(), labels.cpu().numpy()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}